from dotenv import load_dotenv
import openai
import os

# === CONFIGURA√á√ïES ===

load_dotenv()  

openai.api_key = os.getenv("OPENROUTER_API_KEY")  
openai.api_base = "https://openrouter.ai/api/v1"

if not openai.api_key:
    raise ValueError("‚ùå A chave de API do OpenRouter n√£o foi definida. Define a vari√°vel de ambiente OPENROUTER_API_KEY.")

# Caminho para o ficheiro de estat√≠sticas
TXT_PATH = "output/vehicle_counts.txt"

# === FUN√á√ïES ===

def parse_vehicle_stats(txt_path):
    stats = {}
    total = 0

    with open(txt_path, 'r', encoding='utf-8') as file:
        lines = file.readlines()

    current_class = None
    for line in lines:
        line = line.strip()

        if line.startswith("Classe:"):
            current_class = line.split(":")[1].strip()
            stats[current_class] = {}
        elif "- Contagem:" in line:
            stats[current_class]["count"] = int(line.split(":")[1].strip())
        elif "- Percentagem:" in line:
            stats[current_class]["percentage"] = float(line.split(":")[1].strip().replace("%", ""))
        elif "- Confian√ßa m√©dia:" in line:
            stats[current_class]["avg_conf"] = float(line.split(":")[1].strip())
        elif line.startswith("TOTAL DETETADO:"):
            total = int(line.split(":")[1].strip())

    return stats, total


def generate_prompt(stats, total):
    prompt = "Gere um relat√≥rio objetivo e conciso com base nas seguintes estat√≠sticas de contagem de ve√≠culos:\n"
    prompt += f"\nTotal de ve√≠culos detetados: {total}\n"

    for cls, data in stats.items():
        prompt += (
            f"\nClasse: {cls}\n"
            f" - Contagem: {data['count']}\n"
            f" - Percentagem: {data['percentage']}%\n"
            f" - Confian√ßa m√©dia: {data['avg_conf']:.2f}\n"
        )

    prompt += "\nO relat√≥rio deve resumir os dados e indicar observa√ß√µes relevantes."
    return prompt


def gerar_relatorio_llm(prompt, modelo="mistralai/mistral-7b-instruct"):
    try:
        response = openai.ChatCompletion.create(
            model=modelo,
            messages=[{"role": "user", "content": prompt}],
            temperature=0.6,
            max_tokens=400
        )
        return response["choices"][0]["message"]["content"]
    except openai.error.OpenAIError as e:
        return f"‚ùå Erro ao gerar relat√≥rio com o LLM: {str(e)}"


# === EXECU√á√ÉO PRINCIPAL ===

if __name__ == "__main__":
    stats, total = parse_vehicle_stats(TXT_PATH)
    prompt = generate_prompt(stats, total)

    print("\nüîπ PROMPT ENVIADO PARA O LLM:\n")
    print(prompt)

    print("\nüî∏ RELAT√ìRIO GERADO PELO LLM:\n")
    relatorio = gerar_relatorio_llm(prompt)
    print(relatorio)

    # Guarda o relat√≥rio num ficheiro .txt
    relatorio_path = "output/relatorio_gerado.txt"
    with open(relatorio_path, "w", encoding="utf-8") as f:
        f.write(relatorio)
    print(f"\n‚úÖ Relat√≥rio guardado em: {relatorio_path}")
